---
title: "Bayesian Linear Regression Project"
author: "Mary Catherine Morrow, Kayla Mota, Summer Dahlen, Shijie Geng"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## 1. Introduction

Bayesian Linear Regression (BLR) is a conditional data analytical method
for understanding the relationship between independent and dependent
variables based on Bayes Theorem while handling uncertainties with
accuracy and interpretability[@walker2007application]. In the prevalent
perspective, the primary method for estimating parameters in a linear
regression model is the frequentist approach. However, this methodology
defines the probability of uncertain events as the relative frequency of
their occurrence[@hajek1996mises], and the resulting model parameters
are derived exclusively from the observed data [@hajek2009fifteen].
Also, when dealing with a small dataset, the frequentist approach may
distort parameter estimates and affect the validity of statistical tests
[@zyphur2015bayesian]. Unlike frequentist method, BLR is a more flexible
statistical model in that it captures all uncertain variables random
[@klauenberg2015tutorial] and also known to be used for experiments that
have small sample sizes [@sugiarto2021determining] and various data
types [@kruschke2010bayesian]. Moreover, BLR eliminates the need for
p-values, offering richer information on parameters, allowing
simultaneous single change point detection in a multivariate sample
[@seidou2007bayesian].

The BLR is characterized by viewing parameters as random variables,
introducing the concept of prior, likelihood, and posterior
distributions [@permai2018linear; @chen2024positional]. The steps for
creating a BLR can be found in Figure. 1 [@klauenberg2015tutorial]. The
BLR method involves multiple steps-essentially, predicting prior values,
imputting collected data, and obtaining probability in the form of a
distribution (Posterior distribution) which is proportional to prior
distribution multiplied by the likehood function
[@baldwin2017introduction]. In BLR, even when utilizing representative
or informed priors, it is crucial to perform a sensitivity analysis to
assess the impact of different prior specifications on the posterior
results. This aims to ensure that the findings are not dependent on the
choice of prior [@kruschke2021bayesian].Once the experiment is conducted
and the posterior distribution is obtained, a check on  skewness of the
distributions, as well as heteroscedasticity. Although scale mixtures of
normal are often used in regression analysis to capture
heteroscedasticity and outliers, skew-symmetric scale mixtures of normal
(SSSM) can capture asymmetrical in addition, making it more robust to
use in BLR [@rubio2016bayesian]. The posterior distribution is where the
Markov Chain Monte Carlo (MCMC) method involves simulating random draws
from the posterior distribution, allowing for exploration of the
parameter space and estimation of the posterior distribution even in
complex, high-dimensional models [@robert2018accelerating;
@rojas2020lose]. Confirming the convergence of MCMC chains is important
for result reliability. Evidence of convergence provides confidence in
the validity of the samples[@marcotte2018gibbs]. After obtaining the
posterior results, inferences can be used for the prior distribution of
the current/next experiment [@rubio2016bayesian].

![Figure. 1 Bayesian Linear Regression
Workflow](https://lh7-us.googleusercontent.com/cCcxlbdn0-O2wjVBZhbGMAnt6wy0pCX-DfPax7pPjyXhEvNvIxp-F_Iddjq0dEGmdALhlnWY_8euXoHK-TFefuKpGtzTmG9-844USgAX_nwuz02bQE8RQn4pNoK4fi8EhZD2KYC56D78fOAOwcTvZqQ)

To explain, posterior probabilities are assigned to the spectrum of
values that parameters can assume. The more prominent the peak, the more
effective it is as an estimate because a single value has a higher
probability compared to other values. In such cases, the credibility
interval, indicating a 95% range of probable parameter estimates, is
narrower. Conversely, when the peak is less pronounced, other estimates
are also plausible, resulting in a wider credibility interval for the
95% range of probable parameter estimates [@shetty2013evidence;
@gill2002bayesian].

Given the strengths of Bayesian linear regression, it has been used in
many disciplines. Researchers have the option to incorporate past
findings as informative priors, a practice observed in fields such as
construction and biomedicine. This analytical approach produces outcomes
that amalgamate previous results with the data from a present study,
treating it as if all existing datasets were collectively analyzed. An
example of using BLR includes determining correction coefficients to
previously inaccurate concrete mixture formulas to ensure the correct
use and amount of chemicals to prevent and then predict the breakdown of
the concrete [@zgheib2019bayesian]. Another example of BLR is
re-determining traffic-flow rate values for vehicles to ensure traffic
flow and the safe number of vehicles at intersections so that the
previous rate values from over 20 years ago could be updated
[@sugiarto2021determining]. BLR can also be used to determine the load
and strain a bridge can take before it is unsafe to be
used[@zhang2022long]. It also can be helpful to predict genetic values
for genomic selection for plants and animals to prevent the risk of
passing on genes that could cause illnesses [@perez2010genomic].

## 2. Methods

The common non-parametric regression model is
$Y_i = m(X_i) + \varepsilon_i$, where $Y_i$ can be defined as the sum of
the regression function value $m(x)$ for $X_i$. Here $m(x)$ is unknown
and $\varepsilon_i$ some errors. With the help of this definition, we
can create the estimation for local averaging i.e. $m(x)$ can be
estimated with the product of $Y_i$ average and $X_i$ is near to $x$. In
other words, this means that we are discovering the line through the
data points with the help of surrounding data points. The estimation
formula is printed below [@R-base]:

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$ $W_n(x)$ is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if $X_i$ is far from $x$.

Another equation:

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

## 3. Analysis and Results

### 3.1 Data Introduction

The heart disease data set comprises four distinct databases: Cleveland,
Hungary, Switzerland, and Long Beach V. There are a total of 303
patients with the presence of heart disease or not tested by 13 features
contributing to the whole data set. The target variable refers to the
presence of heart disease in the patient. It is integer valued 0 = no
disease and 1,2,3,4 = disease. In subsequent sections, we will conduct
exploratory data analysis, develop a BLR model to predict the target
variable, and assess the model's performance, among other analyses. The
below Table 1 shows the description of the data set.

![Table. 1 Dataset
Description](https://lh7-us.googleusercontent.com/w_TQFxzMVCpOAdKTxw7vxUXzZi_namN5OuK14NUh5fJibDYhYWh8xZujOpoJUKxL0HQsA9OD0c1ZdNGSgfbM0p6jnzsOH7ihk7T6UTp8vkcufQwCVPyu2KkxK9t4WPOAUDuS3OapvsKdzoVarR12aXE){fig-align="center"}

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(gtsummary)
library(skimr)
library(randomForest)
library(GGally)

```

3.1.1 Load the dataset and inspect the first five records.

```{r, warning=FALSE, echo=TRUE}
# Load Data
hd_data<-read.csv("C:/Users/shiji/OneDrive/桌面/capstone data science/processedcleveland.csv", header=FALSE)
names(hd_data) <- c("age", "sex", "cp", "trestbps", "chol", 
                   "fbs", "restecg", "thalach", "exang", "oldpeak", 
                   "slope", "ca", "thal", "num")
head(hd_data)
str(hd_data)
  

```

3.1.2 Transform the continuous-valued variables into numeric format and
convert the categorical variables into factors.

```{r,warning=FALSE, echo=T, message=FALSE}
hd_data$sex <- as.factor(hd_data$sex)
hd_data$cp <- as.factor(hd_data$cp)
hd_data$fbs <- as.factor(hd_data$fbs)
hd_data$restecg <- as.factor(hd_data$restecg)
hd_data$exang <- as.factor(hd_data$exang)
hd_data$slope <- as.factor(hd_data$slope)
hd_data$ca <- as.factor(hd_data$ca)
hd_data$thal <- as.factor(hd_data$thal)
hd_data$num <- as.factor(hd_data$num)
hd_data$chol <- as.numeric(hd_data$chol)
hd_data$age <- as.numeric(hd_data$age)
hd_data$trestbps <- as.numeric(hd_data$trestbps)
hd_data$thalach <- as.numeric(hd_data$thalach)
```

### 3.2 Exploration Data Analyses

3.2.1 Summarize the Dataset

```{r, warning=FALSE, echo=T, message=FALSE}
hd_data %>% 
  tbl_summary(
    by=num,
    label = list(age ~"Age (yrs)", sex~"Sex: Male", 
                 cp ~ "Chest Pain Type",
                 trestbps~ "Resting Blood Pressure (mmHg)", 
                 chol~ "Serum Cholestoral (mg/dl)",
                 fbs~"Fasting Blood Sugar>120 mg/dl", 
                 restecg ~ "Resting Electrocardiographic",
                 exang~"Exercise Induced Angina", 
                 oldpeak~"Exercise Induced ST depression",
                 slope~"Peak Exercise ST Slope", 
                 ca~"Major Vessel Count", 
                 thal~"Defect Presence (3=normal)"),
    missing_text = "(Missing)",
    statistic = list(
      all_continuous()~ "{mean} ({sd})",
      all_categorical()~ "{n}/{N} ({p}%)"
    )
  ) %>%
  add_n()%>%
  add_overall()%>%
  modify_header(label="**Variable**") %>%
  modify_spanning_header(c("stat_2", "stat_3", "stat_4", "stat_5")~ "**Heart Disease Present**")%>%
  modify_spanning_header(c("stat_1")~ "**Heart Disease Absent**") %>%
  modify_caption("**Table 2. Patient Characteristics**") %>%
  modify_footnote(all_stat_cols()~ "Mean (SD) or n/N(%)") %>%
  bold_labels()

```

```{r}
skim(hd_data)
```

As observed, the summary provides a comprehensive overview of the
dataset's structure. Table 2 presents the distribution of patients
across various levels of heart disease, along with the mean and standard
deviation of numeric variables, and the proportions of different levels
in categorical variables. Additionally, the second summary offers
descriptive statistics, including the presence of missing values in the
"Ca" and "thal" variables, as well as the five quantiles of numeric
variables and their histogram distributions. However, due to the broad
range of each bin, it becomes challenging to discern finer details of
these numeric variables. Consequently, the next step involves refining
the bin sizes to explore additional information.

3.2.2 Outliers Detection of Numeric Variables

```{r}
par(mfrow = c(2,5))
hist(hd_data$age, breaks = 45, col = 'lightblue', xlab = 'Age', main = 'Histogram of Age')
boxplot(hd_data$age, main = 'Boxplot of Age')
hist(hd_data$trestbps, breaks = 45, col = 'lightblue', xlab = 'trestbps', main = 'Histogram of trestbps' )
boxplot(hd_data$trestbps, main = 'Boxplot of trestbps')
hist(hd_data$chol, breaks = 45, col = 'lightblue',xlab = 'chol', main = 'Histogram of chol')
boxplot(hd_data$chol, main = 'Boxplot of chol')
hist(hd_data$thalach , breaks =45, col = 'lightblue',xlab = 'thalach', main = 'Histogram of thalach')
boxplot(hd_data$thalach, main = 'Boxplot of thalach')
hist(hd_data$oldpeak, breaks = 45, col = 'lightblue', xlab = 'oldpeak', main = 'Histogram of oldpeak')
boxplot(hd_data$oldpeak, main = 'Boxplot of oldpeak')

# Identify the outliers and replace the outliers with NA values

hd_data$trestbps[which(hd_data$trestbps > 170)] <- NA
hd_data$chol[which(hd_data$chol >380)] <- NA
hd_data$oldpeak[which(hd_data$oldpeak > 4)] <- NA
```

The visualizations indicate that certain values on the right side of the
histograms for 'trestbps', 'chol', and 'oldpeak' are noticeably
different from the main distribution. Additionally, it's easier to
identify points above the fifth quantile in the boxplots for these three
variables. These points represent outliers, which have been replaced
with NA values.

3.2.3 Missing Values Imputation

```{r}
# Recategorize the target variable and modify its data type.
hd_data$num <- ifelse(hd_data$num == 0, 'no_hd', 'hd')
hd_data$num <- as.factor(hd_data$num)

# Missing Values Imputation
set.seed(123)
hd_data.Imputed <- rfImpute(num~., data = hd_data, iter = 6)
```

In the preceding step, the rfImpute function is utilized to fill in new
values for the missing data. The Out-of-Bag error falls within the range
of 15% to 18%, which indicates relatively good performance.

3.2.4 Verify the correlation among variables.

```{r,warning=FALSE,message=FALSE,echo=T}
hd_data.Imputed %>%  ggpairs(mapping = aes(color = num),
          columns = 2:14,
          upper = list(continuous = "cor", discrete = "count", na = "na"),
          lower = list(continuous = "points", discrete = "facetbar", na =
    "na"),
          diag = list(continuous = "densityDiag",  na = "naDiag"),
          )
  
```

The visualization illustrates the correlation coefficients among the
numeric variables. Among these, three pairs of numerical
variables---trestbps and thalach, chol and thalach, chol and
oldpeak---show insignificant correlation. Conversely, the remaining
pairs of numerical variables exhibit significant correlation.

3.2.5 Data Visualization

```{r}
hd_data.Imputed %>% 
  ggplot(aes(x = num, y = age, fill = cp)) + 
  geom_boxplot(alpha = 0.5) + 
  facet_wrap(~ sex, scales = 'free')+
  xlab('HeartDisease') +
  theme_bw()
```

Based on the visualization, patients' ages span from 45 to 65 years. The
age of patients with the heart disease presence is relatively higher
than the other group, but not significantly. The median age of patients
with heart disease in male and female groups is around 60 years old, the
patients without heart disease is a little lower than 60 years old.
Compared to the 4 levels chest pain groups, the median age of patients
with atypical angina but without heart disease is lower than the other
three groups, and the median age of patients with typical angina but
without heart disease is the highest.

```{r, warning=FALSE, message=FALSE}
hd_data.Imputed %>% group_by(num, restecg) %>% summarise(trestbps = mean(trestbps),
                                                         oldpeak = mean(oldpeak)) %>%  
  ggplot(aes(x = restecg, y = trestbps, color = num)) + 
  geom_point(aes(size = oldpeak)) +
  xlab('resting electrocardiographic results ') +
  ylab('resting blood pressure') +
  theme_bw()
```

This graph reveals the relationship among trestbps, oldpeak and restecg.
Patients with level 1 restecg tends to appeare higher resting blood
pressure. The resting blood pressure of patients without heart disease
is lower that patients with heart disease. Moreover, patients with heart
disease usually have higher level oldpeak. \### Conclusion

## References
